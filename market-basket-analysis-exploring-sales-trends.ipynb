{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Welcome to this notebook where we will explore the fascinating world of Market Basket Analysis using a real-world dataset. Market Basket Analysis is a powerful technique that allows us to uncover patterns and associations between items that customers tend to purchase together. By analyzing these patterns, we can gain valuable insights that can drive business decisions and strategies.\n\nIn this notebook, we will work with a Market Basket dataset that captures customer transactions in a retail or e-commerce setting. The dataset provides a wealth of information about customer purchases, allowing us to dive deep into their buying behavior. By leveraging data mining techniques and association rule mining algorithms, we will unravel the relationships between items and discover interesting patterns.\n\nThrough this analysis, we can derive actionable insights to improve various aspects of business operations. We can identify frequently co-purchased items, enabling us to make targeted product recommendations and enhance cross-selling and upselling opportunities. By optimizing product placement and store layout based on association patterns, we can create more enticing shopping experiences. Furthermore, we can design effective promotional campaigns by leveraging the discovered item associations, resulting in higher customer engagement and increased sales.\n\nIn this notebook, we will take you through the entire process of Market Basket Analysis, from data preprocessing to association rule mining and visualization. By following along with the provided code and explanations, you will gain a solid understanding of how to extract valuable insights from Market Basket datasets and apply them to real-world scenarios.\n\nSo let's dive in and unlock the secrets hidden within the Market Basket dataset to gain a deeper understanding of customer behavior and optimize business strategies!","metadata":{}},{"cell_type":"markdown","source":"## Overview of the Market Basket Analysis dataset","metadata":{}},{"cell_type":"markdown","source":"This dataset contains 522,065 rows and 7 attributes that provide valuable information about customer transactions and product details. Here is a breakdown of the attributes:\n\n    BillNo: This attribute represents a 6-digit number assigned to each transaction. It serves as a unique identifier for identifying individual purchases.\n\n    Itemname: This attribute stores the name of the product purchased in each transaction. It provides nominal data representing different products.\n\n    Quantity: This attribute captures the quantity of each product purchased in a transaction. It is a numeric value that indicates the number of units of a specific item.\n\n    Date: The Date attribute records the day and time when each transaction occurred. It provides valuable information about the timing of purchases.\n\n    Price: This attribute represents the price of each product. It is a numeric value that indicates the cost of a single unit of the item.\n\n    CustomerID: Each customer is assigned a 5-digit number as their unique identifier. This attribute helps track customer-specific information and analyze individual buying patterns.\n\n    Country: The Country attribute denotes the name of the country where each customer resides. It provides nominal data representing different geographic regions.\n\nBy analyzing this dataset, we can gain insights into customer purchasing behavior, identify popular products, examine sales trends over time, and explore the impact of factors such as price and geography on customer preferences. These insights can be used to optimize marketing strategies, improve inventory management, and enhance customer satisfaction.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np  # Import numpy library for efficient array operations\nimport pandas as pd  # Import pandas library for data processing\nimport matplotlib.pyplot as plt  # Import matplotlib.pyplot for data visualization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-29T19:17:00.190433Z","iopub.execute_input":"2023-05-29T19:17:00.190901Z","iopub.status.idle":"2023-05-29T19:17:00.225481Z","shell.execute_reply.started":"2023-05-29T19:17:00.190863Z","shell.execute_reply":"2023-05-29T19:17:00.224504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading\nRetrieving and Loading the Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/market-basket-analysis/Assignment-1_Data.csv', sep=';',parse_dates=['Date'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:00.227106Z","iopub.execute_input":"2023-05-29T19:17:00.228031Z","iopub.status.idle":"2023-05-29T19:17:03.848101Z","shell.execute_reply.started":"2023-05-29T19:17:00.228001Z","shell.execute_reply":"2023-05-29T19:17:03.847072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'Price' column to float64 data type after replacing commas with dots\ndf['Price'] = df['Price'].str.replace(',', '.').astype('float64')","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:03.849516Z","iopub.execute_input":"2023-05-29T19:17:03.850167Z","iopub.status.idle":"2023-05-29T19:17:04.309204Z","shell.execute_reply.started":"2023-05-29T19:17:03.850136Z","shell.execute_reply":"2023-05-29T19:17:04.308069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the information about the DataFrame which is to provide an overview of the DataFrame's structure and column data types.\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:04.312165Z","iopub.execute_input":"2023-05-29T19:17:04.312482Z","iopub.status.idle":"2023-05-29T19:17:04.803644Z","shell.execute_reply.started":"2023-05-29T19:17:04.312453Z","shell.execute_reply":"2023-05-29T19:17:04.802498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of missing values for each column and sort them in descending order\ndf.isna().sum().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:04.804940Z","iopub.execute_input":"2023-05-29T19:17:04.805255Z","iopub.status.idle":"2023-05-29T19:17:05.296071Z","shell.execute_reply.started":"2023-05-29T19:17:04.805227Z","shell.execute_reply":"2023-05-29T19:17:05.295005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the total price by multiplying the quantity and price columns\ndf['Total_Price'] = df.Quantity * df.Price","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:05.297318Z","iopub.execute_input":"2023-05-29T19:17:05.297760Z","iopub.status.idle":"2023-05-29T19:17:05.304924Z","shell.execute_reply.started":"2023-05-29T19:17:05.297730Z","shell.execute_reply":"2023-05-29T19:17:05.304023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:05.306238Z","iopub.execute_input":"2023-05-29T19:17:05.307081Z","iopub.status.idle":"2023-05-29T19:17:05.861179Z","shell.execute_reply.started":"2023-05-29T19:17:05.307035Z","shell.execute_reply":"2023-05-29T19:17:05.860119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the number of unique countries in the 'Country' column\nprint(\"Number of unique countries:\", df['Country'].nunique())\n\n# Calculate and print the normalized value counts of the top 5 countries in the 'Country' column\nprint(df['Country'].value_counts(normalize=True)[:5])","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:05.862382Z","iopub.execute_input":"2023-05-29T19:17:05.862903Z","iopub.status.idle":"2023-05-29T19:17:06.000652Z","shell.execute_reply.started":"2023-05-29T19:17:05.862873Z","shell.execute_reply":"2023-05-29T19:17:05.999597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering that the majority of transactions (approximately 93%) in the dataset originate from the UK, the 'Country' column may not contribute significant diversity or variability to the analysis. Therefore, we can choose to remove the 'Country' column from the DataFrame df. we indicate that we want to drop a column, This step allows us to focus on other attributes that may provide more valuable insights for our analysis.","metadata":{}},{"cell_type":"code","source":"# Delete the 'Country' column from the DataFrame\ndf.drop('Country', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:06.002157Z","iopub.execute_input":"2023-05-29T19:17:06.002787Z","iopub.status.idle":"2023-05-29T19:17:06.032817Z","shell.execute_reply.started":"2023-05-29T19:17:06.002758Z","shell.execute_reply":"2023-05-29T19:17:06.031874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the DataFrame to display rows where 'BillNo' column contains non-digit values\ndf[df['BillNo'].str.isdigit() == False]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:06.038434Z","iopub.execute_input":"2023-05-29T19:17:06.038777Z","iopub.status.idle":"2023-05-29T19:17:06.815893Z","shell.execute_reply.started":"2023-05-29T19:17:06.038750Z","shell.execute_reply":"2023-05-29T19:17:06.814788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the item name \"Adjust bad debt\" was filled accidentally and does not provide any useful information for our analysis, we can choose to remove the corresponding rows from the DataFrame. The code snippet above filters the DataFrame df to retain only the rows where the 'Itemname' column does not contain the value \"Adjust bad debt\". This operation effectively eliminates the rows associated with the accidental data entry, ensuring the dataset is free from this irrelevant item name.","metadata":{}},{"cell_type":"code","source":"# Remove rows where the 'Itemname' column contains \"Adjust bad debt\"\ndf = df[df['Itemname'] != \"Adjust bad debt\"]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:06.819420Z","iopub.execute_input":"2023-05-29T19:17:06.819734Z","iopub.status.idle":"2023-05-29T19:17:06.938289Z","shell.execute_reply.started":"2023-05-29T19:17:06.819708Z","shell.execute_reply":"2023-05-29T19:17:06.937084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here to check if all BillNo doesn't inculde letters \ndf['BillNo'].astype(\"int64\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:06.939629Z","iopub.execute_input":"2023-05-29T19:17:06.939965Z","iopub.status.idle":"2023-05-29T19:17:06.999703Z","shell.execute_reply.started":"2023-05-29T19:17:06.939934Z","shell.execute_reply":"2023-05-29T19:17:06.998602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the sum of 'Price' for rows where 'Itemname' is missing\ndf[df['Itemname'].isna()] ['Price'].sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.001180Z","iopub.execute_input":"2023-05-29T19:17:07.001554Z","iopub.status.idle":"2023-05-29T19:17:07.071382Z","shell.execute_reply.started":"2023-05-29T19:17:07.001510Z","shell.execute_reply":"2023-05-29T19:17:07.070295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploring Rows with Missing Item Names:\n\nTo investigate the data where the 'Itemname' column has missing values, we can filter the dataset to display only those rows. This subset of the data will provide insights into the records where the item names are not available.","metadata":{}},{"cell_type":"code","source":"# Filter the DataFrame to display rows where 'Itemname' is missing\ndf[df['Itemname'].isna()]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.073162Z","iopub.execute_input":"2023-05-29T19:17:07.073620Z","iopub.status.idle":"2023-05-29T19:17:07.151603Z","shell.execute_reply.started":"2023-05-29T19:17:07.073581Z","shell.execute_reply":"2023-05-29T19:17:07.150718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon examining the data where the 'Itemname' column has missing values, it becomes evident that these missing entries do not contribute any meaningful information. Given that the item names are not available for these records, it suggests that these instances may not be crucial for our analysis. As a result, we can consider these missing values as non-significant and proceed with our analysis without incorporating them.","metadata":{}},{"cell_type":"code","source":"# Filter the DataFrame to exclude rows where 'Itemname' is missing (not NaN)\ndf = df[df['Itemname'].notna()]\n\n# Print the number of unique items in the 'Itemname' column\nprint(\"Number of unique items:\", df['Itemname'].nunique())\n\n# Calculate and print the normalized value counts of the top 5 items in the 'Itemname' column\nprint(df['Itemname'].value_counts(normalize=True)[:5])","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.152914Z","iopub.execute_input":"2023-05-29T19:17:07.153400Z","iopub.status.idle":"2023-05-29T19:17:07.399238Z","shell.execute_reply.started":"2023-05-29T19:17:07.153371Z","shell.execute_reply":"2023-05-29T19:17:07.398122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A curious observation has caught our attentionâ€”the presence of a negative quantity in the 515,623rd row. ","metadata":{}},{"cell_type":"markdown","source":"we are intrigued by the existence of negative quantities within the dataset. To gain a deeper understanding of this phenomenon, we focus our attention on these specific instances and aim to uncover the underlying reasons behind their occurrence. Through this exploration, we expect to gain valuable insights into the nature of these negative quantities and their potential impact on our analysis. Our investigation aims to reveal the intriguing stories that lie within this aspect of the data.","metadata":{}},{"cell_type":"code","source":"# Filter the DataFrame to display rows where 'Quantity' is less than 1\ndf[df['Quantity'] < 1]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.400430Z","iopub.execute_input":"2023-05-29T19:17:07.400760Z","iopub.status.idle":"2023-05-29T19:17:07.422870Z","shell.execute_reply.started":"2023-05-29T19:17:07.400734Z","shell.execute_reply":"2023-05-29T19:17:07.421775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given the observation that negative quantities might be filled with system issues or irrelevant information for our analysis, it is reasonable to proceed with removing these rows from the dataset. By doing so, we can ensure the accuracy and reliability of our data, as well as eliminate potential biases or misleading information stemming from negative quantities.","metadata":{}},{"cell_type":"code","source":"# Remove rows where 'Quantity' is less than 1\ndf = df[df['Quantity'] >= 1]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.424455Z","iopub.execute_input":"2023-05-29T19:17:07.425299Z","iopub.status.idle":"2023-05-29T19:17:07.465546Z","shell.execute_reply.started":"2023-05-29T19:17:07.425259Z","shell.execute_reply":"2023-05-29T19:17:07.464318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we turn our attention to the presence of missing values in the 'CustomerID' column. By investigating these missing values, we aim to identify any potential issues or data quality concerns associated with them. Analyzing the impact of missing 'CustomerID' values will help us assess the completeness and reliability of the dataset, enabling us to make informed decisions on handling or imputing these missing values. Let's dive deeper into this aspect and gain a comprehensive understanding of any issues related to missing 'CustomerID' values.","metadata":{}},{"cell_type":"code","source":"# Select a random sample of 30 rows where 'CustomerID' is missing\ndf[df['CustomerID'].isna()].sample(30)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.467059Z","iopub.execute_input":"2023-05-29T19:17:07.467497Z","iopub.status.idle":"2023-05-29T19:17:07.505313Z","shell.execute_reply.started":"2023-05-29T19:17:07.467460Z","shell.execute_reply":"2023-05-29T19:17:07.504079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This sample can provide us with a glimpse into the specific instances where 'CustomerID' is missing, aiding us in further analysis or decision-making related to handling these missing values.","metadata":{}},{"cell_type":"markdown","source":"Upon analyzing a sample of rows where the 'CustomerID' is missing, it appears that there is no discernible pattern or specific reason behind the absence of these values. This observation suggests that the missing 'CustomerID' entries were not filled accidentally or due to a systematic issue. Instead, it is possible that these missing values occur naturally in the dataset, without any particular significance or underlying cause.","metadata":{}},{"cell_type":"markdown","source":"#### Identifying Issues in the Price Column: Ensuring Data Quality\nIn our analysis, we shift our focus to the 'Price' column and investigate it for any potential issues or anomalies. By thoroughly examining the data within this column, we aim to identify any irregularities, inconsistencies, or outliers that may affect the overall quality and integrity of the dataset. Analyzing the 'Price' column is crucial in ensuring accurate and reliable pricing information for our analysis. Let's dive deeper into the 'Price' column and uncover any issues that may require attention.","metadata":{}},{"cell_type":"code","source":"# Counting the number of rows where the price is zero\nzero_price_count = len(df[df['Price'] == 0])\nprint(\"Number of rows where price is zero:\", zero_price_count)\n\n# Counting the number of rows where the price is negative\nnegative_price_count = len(df[df['Price'] < 0])\nprint(\"Number of rows where price is negative:\", negative_price_count)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.506921Z","iopub.execute_input":"2023-05-29T19:17:07.507343Z","iopub.status.idle":"2023-05-29T19:17:07.516671Z","shell.execute_reply.started":"2023-05-29T19:17:07.507300Z","shell.execute_reply":"2023-05-29T19:17:07.515875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"our attention now turns to the presence of zero charges in the 'Price' column. It is important to explore instances where products were offered free of cost, as this information can provide valuable insights into promotional activities, giveaways, or other unique aspects of the dataset. By examining the data related to zero charges in the 'Price' column, we can gain a deeper understanding of these transactions and their potential impact on our analysis. Let's delve into the details of these zero-priced transactions and uncover any significant findings.","metadata":{}},{"cell_type":"code","source":"# Selecting a random sample of 20 rows where the price is zero\ndf[df['Price'] == 0].sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.518049Z","iopub.execute_input":"2023-05-29T19:17:07.518631Z","iopub.status.idle":"2023-05-29T19:17:07.543891Z","shell.execute_reply.started":"2023-05-29T19:17:07.518601Z","shell.execute_reply":"2023-05-29T19:17:07.542880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing Rows with Zero Price: Eliminating Misleading Data Entries\n\nUpon reviewing the sample of rows where the price is zero, we have identified that these entries might provide misleading or inaccurate information for our analysis. Therefore, it is prudent to proceed with removing these rows from the dataset to ensure the integrity and reliability of our analysis.","metadata":{}},{"cell_type":"code","source":"# Remove rows where the price is zero\ndf = df[df['Price'] != 0]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.545249Z","iopub.execute_input":"2023-05-29T19:17:07.545547Z","iopub.status.idle":"2023-05-29T19:17:07.579973Z","shell.execute_reply.started":"2023-05-29T19:17:07.545509Z","shell.execute_reply":"2023-05-29T19:17:07.578870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding: Exploring and Interpreting the Dataset","metadata":{}},{"cell_type":"markdown","source":"In the data analysis process, data understanding plays a crucial role in gaining insights and formulating meaningful conclusions. By thoroughly examining the dataset, we aim to understand its structure, contents, and underlying patterns. This understanding empowers us to make informed decisions regarding data cleaning, feature engineering, and subsequent analysis steps.\n\nKey aspects of data understanding include:\n\n    Exploring the Dataset: We investigate the dataset's dimensions, such as the number of rows and columns, to gauge its size and complexity. Additionally, we examine the data types of each column to understand the nature of the variables.\n\n    Assessing Data Quality: We scrutinize the data for inconsistencies, outliers, or other data quality issues that may require attention. Addressing these issues ensures the reliability and accuracy of the data.\n\n    Identifying Relationships: We analyze the relationships between variables by examining correlations, associations, or dependencies. This analysis allows us to uncover meaningful connections that can drive insights and guide our analysis.\n\n    Detecting Patterns and Trends: We look for recurring patterns, trends, or distributions within the data. This step can reveal valuable information about customer behavior, market dynamics, or other relevant factors.\n\nBy thoroughly understanding the dataset, we lay the foundation for meaningful data analysis and generate insights that contribute to informed decision-making and problem-solving.","metadata":{}},{"cell_type":"code","source":"# Grouping the data by month and summing the total price for the year 2010\ndf[df[\"Date\"].dt.year == 2010].groupby(df[\"Date\"].dt.month)[\"Total_Price\"].sum().plot()\n\n# Grouping the data by month and summing the total price for the year 2011\ndf[df[\"Date\"].dt.year == 2011].groupby(df[\"Date\"].dt.month)[\"Total_Price\"].sum().plot()\n\n# Adding legend and plot labels\nplt.legend([\"2010\", \"2011\"])\nplt.title(\"Income over time\")\nplt.ylabel('Total Income (Million)')\nplt.xlabel(\"Date (Month)\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:07.584026Z","iopub.execute_input":"2023-05-29T19:17:07.584373Z","iopub.status.idle":"2023-05-29T19:17:08.227848Z","shell.execute_reply.started":"2023-05-29T19:17:07.584343Z","shell.execute_reply":"2023-05-29T19:17:08.226781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet above creates a line plot to visualize the income over time for the years 2010 and 2011. First, the data is filtered based on the year using the dt.year attribute of the 'Date' column. The data is then grouped by month, and the 'Total_Price' column is summed. Two line plots are created, one for each year, showing the monthly total income. The legend is added to indicate the respective years, and the plot is labeled with a title, y-axis label, and x-axis label. This visualization allows us to observe the trend and compare the income between the two years.","metadata":{}},{"cell_type":"markdown","source":"Upon observing the line plot of income over time for the years 2010 and 2011, it becomes apparent that the sales remained relatively stable and consistent until October 2010. This suggests that the business was growing steadily during this period, as the sales continued to increase.\n\nHowever, a significant drop in sales is observed in the last month of the dataset. This sudden decline indicates a notable deviation from the previously observed growth trend. Exploring the potential factors contributing to this drop becomes crucial in understanding the underlying reasons for the decline in sales during that specific period.","metadata":{}},{"cell_type":"markdown","source":"To verify if the data is complete for the entire last month in the dataset, we can compare the maximum date in the 'Date' column with the last day of that month. If they match, it indicates that the data is filled for the entire last month.","metadata":{}},{"cell_type":"code","source":"df[\"Date\"].max()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:08.229588Z","iopub.execute_input":"2023-05-29T19:17:08.230277Z","iopub.status.idle":"2023-05-29T19:17:08.238968Z","shell.execute_reply.started":"2023-05-29T19:17:08.230237Z","shell.execute_reply":"2023-05-29T19:17:08.237917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the finding that the data is only available for 10 days in the last month, it becomes evident that the significant drop in sales observed during that period is likely due to the limited data rather than an actual decline in sales. The incomplete data for the last month may not provide a comprehensive representation of the sales performance during that period.\n\nTo gain a more accurate understanding of the sales trend, it is advisable to consider a broader time frame with complete data. Analyzing a more extended period that encompasses multiple months or years would provide a more reliable assessment of the sales performance and allow for more meaningful insights and conclusions.","metadata":{}},{"cell_type":"code","source":"# Plotting the top 10 most sold products by quantity\ndf.groupby('Itemname')['Quantity'].sum().sort_values(ascending=False)[:10].plot(kind='barh', title='Number of Quantity Sold')\nplt.ylabel('Item Name')\nplt.xlim(20000, 82000)\nplt.show()\n\n# Plotting the top 10 most sold products by count\ndf['Itemname'].value_counts(ascending=False)[:10].plot(kind='barh', title='Number of Sales')\nplt.ylabel('Item Name')\nplt.xlim(1000, 2300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:08.240261Z","iopub.execute_input":"2023-05-29T19:17:08.240802Z","iopub.status.idle":"2023-05-29T19:17:08.893112Z","shell.execute_reply.started":"2023-05-29T19:17:08.240756Z","shell.execute_reply":"2023-05-29T19:17:08.892068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet above creates two horizontal bar plots to visualize the most sold products based on quantity and count, respectively.\n\nIn the first plot, the top 10 items are determined by summing the 'Quantity' column for each unique 'Itemname' and sorting them in descending order. The plot displays the number of quantities sold for each item.\n\nThe second plot showcases the top 10 items based on the count of sales for each unique 'Itemname'. The value_counts function counts the occurrences of each item and sorts them in descending order. The plot represents the number of times each item has been sold.","metadata":{}},{"cell_type":"markdown","source":"Observing the plots, we can infer that there are products that are sold more frequently (higher count) compared to others, despite having relatively lower quantities sold per transaction. This indicates the presence of items that are commonly purchased in larger quantities at once. These products might include items that are frequently bought in bulk or items that are typically sold in larger packages or quantities.\n\nThis insight highlights the importance of considering both the quantity sold and the count of sales when analyzing the popularity and demand for different products. It suggests that some items may have a higher turnover rate due to frequent purchases, while others may have a higher quantity per sale, leading to different sales patterns and customer behaviors. Understanding these dynamics can be valuable for inventory management, pricing strategies, and identifying customer preferences.","metadata":{}},{"cell_type":"markdown","source":"# Association Rules","metadata":{}},{"cell_type":"markdown","source":"Association rules are generated using the Apriori algorithm, which is a popular algorithm for discovering interesting relationships or associations among items in a dataset. Association rule mining is commonly used in market basket analysis, where the goal is to find associations between items frequently purchased together.\n\nThe generated association rules provide insights into the relationships between different items or itemsets in the dataset. Each association rule consists of two parts: the antecedent (or left-hand side) and the consequent (or right-hand side). The antecedent represents the item(s) or itemset(s) that act as a condition or premise, while the consequent represents the item(s) or itemset(s) that are predicted or inferred from the antecedent.\n\nThe association rules are evaluated based on different metrics, such as support, confidence, lift, leverage, and conviction. These metrics provide measures of the interestingness or strength of the rules. \n\n- Support measures the proportion of transactions in the dataset that contain both the antecedent and the consequent.\n- Confidence measures the conditional probability of the consequent given the antecedent.\n- Lift measures the ratio of observed support to expected support, indicating the strength of the association between the antecedent and the consequent.\n- Leverage measures the difference between the observed support and the expected support, indicating the significance of the association.\n- Conviction measures the ratio of the expected confidence to the observed confidence, indicating the degree of dependency between the antecedent and the consequent.\n\nBy examining the association rules, you can identify interesting relationships, co-occurrences, or patterns among items, which can be used for various purposes such as product recommendation, market segmentation, or inventory management.\n\nTo generate the association rules, we use the Apriori algorithm with a minimum support threshold of 0.05 (5%). This ensures that only itemsets with sufficient frequency in the dataset are considered.\n\nLet's explore the generated association rules:\n","metadata":{}},{"cell_type":"code","source":"# Assign the original DataFrame to df2\ndf2 = df\n\n# Filter rows based on item occurrences\nitem_counts = df2['Itemname'].value_counts(ascending=False)\nfiltered_items = item_counts.loc[item_counts > 1].reset_index()['index']\ndf2 = df2[df2['Itemname'].isin(filtered_items)]\n\n# Filter rows based on bill number occurrences\nbill_counts = df2['BillNo'].value_counts(ascending=False)\nfiltered_bills = bill_counts.loc[bill_counts > 1].reset_index()['index']\ndf2 = df2[df2['BillNo'].isin(filtered_bills)]","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:08.894625Z","iopub.execute_input":"2023-05-29T19:17:08.894953Z","iopub.status.idle":"2023-05-29T19:17:09.219465Z","shell.execute_reply.started":"2023-05-29T19:17:08.894917Z","shell.execute_reply":"2023-05-29T19:17:09.218307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Filtering is done based on item occurrences:\n        The frequency count of each unique item name in the 'Itemname' column is calculated and stored in item_counts.\n        filtered_items is created by filtering item_counts to retain only item names that occur more than once.\n        Rows in df2 are filtered to keep only those where the item name in the 'Itemname' column is present in the filtered_items list.\n#### Filtering is done based on bill number occurrences:\n        The frequency count of each unique bill number in the 'BillNo' column is calculated and stored in bill_counts.\n        filtered_bills is created by filtering bill_counts to retain only bill numbers that occur more than once.\n        Rows in df2 are filtered to keep only those where the bill number in the 'BillNo' column is present in the filtered_bills list.\n\nAfter executing the code, the filtered DataFrame df2 will contain only the rows where both the item name and bill number occur more than once in the original df.","metadata":{}},{"cell_type":"code","source":"# Create a pivot table using the filtered DataFrame\npivot_table = pd.pivot_table(df2[['BillNo','Itemname']], index='BillNo', columns='Itemname', aggfunc=lambda x: True, fill_value=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:09.221130Z","iopub.execute_input":"2023-05-29T19:17:09.221845Z","iopub.status.idle":"2023-05-29T19:17:52.509345Z","shell.execute_reply.started":"2023-05-29T19:17:09.221804Z","shell.execute_reply":"2023-05-29T19:17:52.508369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code creates a pivot table that represents the occurrence of items in bills. The pivot table provides a binary representation where each cell indicates whether a specific item appears in a particular bill. Here's how it works:\n\n    The original DataFrame df2 contains information about bills and corresponding item names.\n    By using the pd.pivot_table() function, we reshape the DataFrame to create a pivot table.\n    The pivot table has 'BillNo' as the index and 'Itemname' as the columns, grouping the data based on these two columns.\n    The goal is to determine whether a specific item appears in a particular bill.\n    Each cell in the pivot table is filled with either True or False:\n        If an item appears in a bill, the corresponding cell is marked as True.\n        If an item does not appear in a bill, the corresponding cell is marked as False.\n    This binary representation of item occurrence in bills allows us to easily analyze and identify patterns or associations between different items and bills.\n\nThe resulting pivot table provides a concise summary of the occurrence of items in bills, which can be used for various purposes such as market basket analysis, recommendation systems, or identifying frequent itemsets and association rules.","metadata":{}},{"cell_type":"code","source":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\n# Generate frequent itemsets with minimum support of 0.1 (10%)\nfrequent_itemsets = apriori(pivot_table, min_support=0.01,use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, \"confidence\", min_threshold = 0.5)\n\n# Print frequent itemsets\nprint(\"Frequent Itemsets:\")\nprint(frequent_itemsets)\n\n# Print association rules\nprint(\"\\nAssociation Rules:\")\nrules","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:17:52.510931Z","iopub.execute_input":"2023-05-29T19:17:52.511364Z","iopub.status.idle":"2023-05-29T19:18:16.067626Z","shell.execute_reply.started":"2023-05-29T19:17:52.511326Z","shell.execute_reply":"2023-05-29T19:18:16.066596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code uses the apriori algorithm and association rule mining techniques to analyze the occurrence of items in bills. Here's the overall idea:\n\n    Frequent Itemsets Generation:\n        The apriori algorithm is applied to the pivot_table created earlier, which represents the occurrence of items in bills.\n        The algorithm identifies sets of items that frequently co-occur together in the bills.\n        The minimum support threshold of 0.01 (1%) is set, meaning that an itemset must occur in at least 1% of the bills to be considered frequent.\n        The resulting frequent itemsets represent combinations of items that are frequently observed together in bills.\n\n    Association Rules Generation:\n        Using the frequent itemsets, association rules are generated.\n        Association rules capture relationships and patterns between items based on their co-occurrence in bills.\n        The confidence metric is used to evaluate the strength of the rules. Confidence measures how often the consequent item(s) appear in bills when the antecedent item(s) are present.\n        A minimum confidence threshold of 0.5 (50%) is set, meaning that only rules with a confidence greater than or equal to 0.5 will be considered significant.\n\nBy applying these techniques to the pivot_table, the code enables the discovery of frequent itemsets and the extraction of meaningful association rules, helping to uncover hidden patterns and relationships in the data.","metadata":{}},{"cell_type":"code","source":"rules = rules.sort_values(['confidence', 'lift'], ascending =[False, False]) \n\nrules","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:18:16.073935Z","iopub.execute_input":"2023-05-29T19:18:16.074251Z","iopub.status.idle":"2023-05-29T19:18:16.108568Z","shell.execute_reply.started":"2023-05-29T19:18:16.074225Z","shell.execute_reply":"2023-05-29T19:18:16.107355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rules.sort_values(by='support', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:18:16.109998Z","iopub.execute_input":"2023-05-29T19:18:16.110334Z","iopub.status.idle":"2023-05-29T19:18:16.141817Z","shell.execute_reply.started":"2023-05-29T19:18:16.110304Z","shell.execute_reply":"2023-05-29T19:18:16.140765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort rules by support in descending order\nsorted_rules = rules.sort_values(by='support', ascending=False)\n\n# Calculate cumulative support\ncumulative_support = np.cumsum(sorted_rules['support'] / np.sum(sorted_rules['support']) * 100)\n\n# Bar plot for Support\nfig, ax1 = plt.subplots(figsize=(8, 6))\nax1.bar(range(len(sorted_rules)), sorted_rules['support'], align='center')\nplt.xticks(range(len(sorted_rules)), ['' for _ in range(len(sorted_rules))])  # Remove x-axis labels\nax1.set_xlabel('Association Rule')\nax1.set_ylabel('Support')\nax1.set_title('Support of Association Rules')\n\n# CDF plot for cumulative support\nax2 = ax1.twinx()\nax2.plot(range(len(sorted_rules)), cumulative_support, color='#AA4A44', linestyle='--')\nax2.set_ylabel('Cumulative Support (%)', c='#AA4A44')\n\nplt.tight_layout()\nplt.show()\n\n# Scatter plot for Confidence vs. Support\nplt.figure(figsize=(8, 6))\nplt.scatter(rules['support'], rules['confidence'], alpha=0.4)\nplt.xlabel('Support')\nplt.ylabel('Confidence')\nplt.title('Confidence vs. Support of Association Rules')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:18:16.143223Z","iopub.execute_input":"2023-05-29T19:18:16.143641Z","iopub.status.idle":"2023-05-29T19:18:23.332687Z","shell.execute_reply.started":"2023-05-29T19:18:16.143612Z","shell.execute_reply":"2023-05-29T19:18:23.331583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These two visualizations explore the association rules: a bar plot for the support of association rules and a scatter plot for the confidence vs. support of association rules.\n\nThe bar plot represents the support values of the association rules. Each bar corresponds to a rule, and its height represents the support value, indicating how frequently the rule occurs in the dataset. The y-axis represents the support, while the x-axis does not display any labels, focusing solely on the visualization of support values.\n\nThe cumulative distribution function (CDF) plot showcases the cumulative support of the association rules as a percentage. It helps understand the distribution of support values across the rules in a cumulative manner. The red dashed line in the CDF plot connects the cumulative support values for each rule, providing insights into the accumulation of support as the rules progress.\n\nThe scatter plot displays the relationship between confidence and support for the association rules. Each point represents a rule, with the x-axis representing the support and the y-axis representing the confidence. The plot shows how the confidence varies with different levels of support, helping identify any patterns or trends between these two metrics.\n\nThese visualizations offer valuable insights into the support, confidence, and their relationships within the association rules, aiding in the interpretation and analysis of the rules' strength and significance.","metadata":{}},{"cell_type":"markdown","source":"## Cross-Selling and Upselling","metadata":{}},{"cell_type":"code","source":"# Filter association rules for cross-selling opportunities\ncross_selling_rules = rules[(rules['antecedents'].apply(len) == 1) & (rules['consequents'].apply(len) == 1)]\n\n# Sort rules based on confidence and support\ncross_selling_rules = cross_selling_rules.sort_values(by=['confidence', 'support'], ascending=False)\n\n# Select top cross-selling recommendations\ntop_cross_selling = cross_selling_rules.head(5)\n\n# Filter association rules for upselling opportunities\nupselling_rules = rules[(rules['antecedents'].apply(len) == 1) & (rules['consequents'].apply(len) > 1)]\n\n# Sort rules based on confidence and support\nupselling_rules = upselling_rules.sort_values(by=['confidence', 'support'], ascending=False)\n\n# Select top upselling recommendations\ntop_upselling = upselling_rules.head(5)\n\n# Display cross-selling recommendations\nprint(\"Cross-Selling Recommendations:\")\nfor idx, row in top_cross_selling.iterrows():\n    antecedent = list(row['antecedents'])[0]\n    consequent = list(row['consequents'])[0]\n    print(f\"Customers who bought '{antecedent}' also bought '{consequent}'.\")\n\n# Display upselling recommendations\nprint(\"\\nUpselling Recommendations:\")\nfor idx, row in top_upselling.iterrows():\n    antecedent = list(row['antecedents'])[0]\n    consequents = list(row['consequents'])\n    print(f\"For customers who bought '{antecedent}', recommend the following upgrades: {', '.join(consequents)}.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:18:23.334221Z","iopub.execute_input":"2023-05-29T19:18:23.335278Z","iopub.status.idle":"2023-05-29T19:18:23.360171Z","shell.execute_reply.started":"2023-05-29T19:18:23.335236Z","shell.execute_reply":"2023-05-29T19:18:23.358952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Upselling Recommendations\n\nDuring the analysis of upselling opportunities, it was observed that multiple product recommendations were being made for the same item. To address this issue and provide more diverse recommendations, a modification was made to recommend only one product for each top item instead of recommending based on the top confidence values.\n\nBy implementing this change, we ensure that the upselling recommendations do not repeatedly suggest the same product to customers. This approach enhances the variety of product recommendations and increases the chances of cross-selling and upselling success.\n\nThe updated recommendation strategy focuses on identifying the top items and selecting a single recommended product for each of them. This adjustment aims to optimize the upselling strategy by suggesting different upgrades or add-on products to customers, resulting in a more compelling and varied range of recommendations.","metadata":{}},{"cell_type":"code","source":"top_upselling = upselling_rules.sort_values(['confidence', 'support'], ascending=False).drop_duplicates('antecedents')[:5]\nfor idx, row in top_upselling.iterrows():\n    antecedent = list(row['antecedents'])[0]\n    consequents = list(row['consequents'])\n    print(f\"For customers who bought '{antecedent}', recommend the following upgrades: {', '.join(consequents)}.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-29T19:18:23.362882Z","iopub.execute_input":"2023-05-29T19:18:23.363322Z","iopub.status.idle":"2023-05-29T19:18:23.372789Z","shell.execute_reply.started":"2023-05-29T19:18:23.363294Z","shell.execute_reply":"2023-05-29T19:18:23.371561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this project, we explored the concept of association rules using the Apriori algorithm and the mlxtend library in Python. Association rules analysis provides valuable insights into the relationships and patterns within a dataset, enabling businesses to uncover hidden associations between items and make informed decisions for various applications.\n\nWe started by preparing the data and filtering out infrequent items and irrelevant transactions. Then, we generated frequent itemsets and association rules based on predefined thresholds for support and confidence. These rules allowed us to identify significant associations between items and quantify their strength.\n\nThe generated association rules provided actionable insights for different business scenarios. We explored cross-selling opportunities by identifying products frequently purchased together. By leveraging these associations, businesses can implement effective cross-selling strategies, offering relevant add-on products or upgrades to customers, thereby increasing revenue.\n\nAdditionally, we examined upselling recommendations, focusing on identifying suitable product upgrades or higher-priced alternatives for customers. By considering only one product recommendation for each top item, we ensured diverse and relevant suggestions, avoiding repetitive recommendations and enhancing the upselling strategy.\n\nFurthermore, we discussed the importance of interpreting the support, confidence, lift, leverage, and conviction metrics associated with association rules. These metrics provide quantitative measures of the strength, significance, and impact of the associations, enabling businesses to prioritize and optimize their decision-making processes.\n\nOverall, association rules analysis offers valuable insights and practical applications across various domains, such as marketing, product recommendations, cross-selling strategies, and process optimization. By understanding the associations between items, businesses can make data-driven decisions, improve customer satisfaction, enhance marketing campaigns, and drive business growth.\n\nIt is important to note that the analysis and insights provided in this project are specific to the dataset and parameters used. The results can be further refined and customized based on the specific requirements, domain knowledge, and business objectives.\n","metadata":{}}]}