{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"editable":false}},{"cell_type":"markdown","source":"In today's world, income is an essential factor that determines the quality of life. Understanding the factors that contribute to high income levels can help individuals, businesses, and governments make better decisions.\n\nIn this Kaggle notebook, we will be exploring the Income Classification dataset from the UC Irvine Machine Learning Repository. The goal of this dataset is to predict whether an individual's income exceeds $50,000 per year or not based on various features such as age, education level, occupation, and more.\n\nThe Income Classification problem is a binary classification task where the target variable is either 0 or 1, representing whether the individual's income is less than or equal to (50,000) or more than    (50,000) respectively.\n\nWe will be using various machine learning techniques to build a model that can accurately predict an individual's income level based on the given features. This problem provides an excellent opportunity to explore different machine learning algorithms and techniques and compare their performance on a real-world dataset.\n\nLet's dive into the data and see what insights we can uncover!","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Libraries and Data Import","metadata":{"editable":false}},{"cell_type":"markdown","source":"To begin, let's import the necessary libraries that we'll be using throughout this notebook:","metadata":{"editable":false}},{"cell_type":"code","source":"# Data Manipulation Libraries\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning Libraries\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier,GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's import the Income Classification dataset from the UC Irvine Machine Learning Repository using the read_csv() function from pandas:","metadata":{"editable":false}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/income-classification/income_evaluation.csv')\ndf.head()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The head() function displays the first 5 rows of the dataset, allowing us to get a sense of what the data looks like. ","metadata":{"editable":false}},{"cell_type":"code","source":"df.info()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The info() function provides information about the dataset, such as the number of non-null values and the data types of each feature. the describe() function gives us summary statistics for each numerical feature in the dataset.","metadata":{"editable":false}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{"editable":false}},{"cell_type":"markdown","source":"Before we can start modeling, we need to clean the Income Classification dataset by handling missing values, and removing duplicates","metadata":{"editable":false}},{"cell_type":"code","source":"# Rename columns with spaces to column names without spaces\ndf.columns = df.columns.str.replace(' ', '')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace \" ?\" values in the dataset with \"Other\"\ndf = df.replace(' ?',' Others')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove duplicate instances\ndf.drop_duplicates(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization","metadata":{"editable":false}},{"cell_type":"markdown","source":"Here, I attempted to use a pie chart to assess the balance of the dataset by visualizing the proportions of each category in the target variable (income level).","metadata":{"editable":false}},{"cell_type":"code","source":"plt.pie(df.income.value_counts(),labels = df.income.unique(),autopct='%1.1f%%')\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"capital_gain = df[df['capital-gain'] > 0]\ncapital_loss = df[df['capital-loss'] > 0]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"capital_gain.tail(3)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I calculated the average value of the 'capital-gain' column to investigate whether non-zero values in this column have an impact on the target variable. By checking the average value, I can get a sense of the distribution of the 'capital-gain' variable and see if it has any correlation with the target variable.","metadata":{"editable":false}},{"cell_type":"code","source":"xx = capital_gain['income'].value_counts().keys()\nyy=[capital_gain[capital_gain['income'] == i]['capital-gain'].mean() for i in xx]\nplt.bar(xx, yy,width = 0.4)\n \nplt.xlabel(\"Income\")\nplt.ylabel(\"Capital Gain\")\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I also calculated the average value of the 'capital-loss' column to investigate if non-zero values in this column have any relationship with the target variable.","metadata":{"editable":false}},{"cell_type":"code","source":"xx = capital_loss['income'].value_counts().keys()\nyy=[capital_loss[capital_loss['income'] == i]['capital-loss'].mean() for i in xx]\nplt.bar(xx, yy,width = 0.4)\n \nplt.title('Capital loss')\nplt.xlabel(\"Income\")\nplt.ylabel(\"Capital Gain\")\nplt.show()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I utilized 'value_counts' function to observe the frequency of each unique value in the 'native-country' column. By doing so, I was able to identify which countries are most common in the dataset and determine whether there are any imbalances in the representation of different countries in the data.","metadata":{"editable":false}},{"cell_type":"code","source":"print(df['native-country'].value_counts(normalize=True)[:5])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I examined the distribution of the 'relationship' column and its relationship with the target variable 'income' using a countplot. By using this plot, I was able to visualize the frequency of each unique value in the 'relationship' column and compare the number of occurrences of each value between different income categories. This plot can help to identify if certain categories of 'relationship' are more likely to be associated with higher or lower incomes.","metadata":{"editable":false}},{"cell_type":"code","source":"sns.countplot(data=df, x=\"relationship\", hue=\"income\")\nplt.xticks(rotation=60)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to compare the distribution of 'marital-status' with the target variable 'income' and see if it is different from that of the 'relationship' column, I created a countplot. This plot is similar to the previous countplot, but the 'marital-status' column is used instead of 'relationship'. By comparing these two plots, it is possible to identify any differences or similarities in the distribution of income levels across different marital statuses and relationship categories. This can provide insights into the potential impact of these variables on the target variable.","metadata":{"editable":false}},{"cell_type":"code","source":"sns.countplot(data=df, x=\"marital-status\", hue=\"income\")\nplt.xticks(rotation=60)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used a countplot to examine the relationship between 'workclass' and 'income' and observe the distribution of income levels across different work classes.","metadata":{"editable":false}},{"cell_type":"code","source":"sns.countplot(data=df, x=\"workclass\", hue=\"income\")\nplt.xticks(rotation=60)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x=\"occupation\", hue=\"income\")\nplt.xticks(rotation=80)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After visualizing the data and identifying the columns that are not useful for the modeling process, I removed those columns from the dataset. This helps to simplify the dataset and prevent irrelevant or redundant information from impacting the model's performance.","metadata":{"editable":false}},{"cell_type":"code","source":"df_new = df.drop(['marital-status','race','fnlwgt', 'education','native-country','workclass'],axis=1)\ndf_new.head()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.head(3)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.select_dtypes(include=np.number).hist(figsize=(8,8))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = pd.cut(df_new['hours-per-week'], bins=[0,20,40,70,100], include_lowest=True, labels=['0-20', '20-40', '40-70','70-100'])\nsns.countplot(x=xx, hue=df[\"income\"])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = pd.cut(df_new['age'], bins=[17,23,40,60,100], include_lowest=True, labels=['17-23', '23-40', '40-60','60-100'])\nsns.countplot(x=xx, hue=df[\"income\"])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"editable":false}},{"cell_type":"markdown","source":"Data encoding to transform categorical data into numerical values.","metadata":{"editable":false}},{"cell_type":"code","source":"df_new['income'].replace({' <=50K':1,' >50K':0},inplace=True)\ndf_new['sex'].replace({' Male':1,' Female':0},inplace=True)\ndf_new.head(2)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_encoded_data = pd.get_dummies(df_new, columns = ['occupation', 'relationship'])\none_hot_encoded_data","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into input features (X) and target variable (y).","metadata":{"editable":false}},{"cell_type":"code","source":"X = one_hot_encoded_data.drop('income',axis=1)\ny = one_hot_encoded_data.income\nX.shape,y.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standardizing the input features using StandardScaler to scale the data.","metadata":{"editable":false}},{"cell_type":"code","source":"scaler = StandardScaler()\n\n# transform data\nX = scaler.fit_transform(X)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the data into training and testing sets using train_test_split.","metadata":{"editable":false}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\nX_train.shape, X_test.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{"editable":false}},{"cell_type":"markdown","source":"For the model building step, I first utilized a decision tree algorithm to create a classifier. To ensure the model's generalization performance, I performed cross-validation.","metadata":{"editable":false}},{"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state=42)\n\nk_folds = KFold(n_splits = 4)\n\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\n\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After using a decision tree model with cross-validation, I decided to try out a different algorithm to see if it would perform better. I chose to use logistic regression, which is a commonly used algorithm for binary classification problems like the one in this dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"clf = LogisticRegression(random_state=0).fit(X_train, y_train)\nclf.score(X_train, y_train),clf.score(X_test, y_test)\n\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\n\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After using logistic regression, I observed that the model performed better than the decision tree algorithm. To further evaluate the model's performance, I utilized a confusion matrix. which can provide insights into the model's performance in terms of accuracy, precision, recall, and F1-score.","metadata":{"editable":false}},{"cell_type":"code","source":"y_pred = clf.predict(X_train)\nconfusion_matrix(y_train, y_pred)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To improve the model's performance, I utilized ensemble learning, which combines the predictions of multiple models to create a single prediction. Specifically, I used a voting classifier that combines the predictions of five different models to produce a final prediction. This approach can improve the model's performance by reducing overfitting and increasing accuracy.","metadata":{"editable":false}},{"cell_type":"code","source":"clf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GradientBoostingClassifier()\nclf4 = XGBClassifier()\nclf5 = AdaBoostClassifier()\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gbc', clf3),('xgb',clf4),('abc',clf5)], voting='soft')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eclf1 = eclf1.fit(X_train, y_train)\ny_pred = eclf1.predict(X_train)\nconfusion_matrix(y_train, y_pred)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{"editable":false}},{"cell_type":"code","source":"scores = cross_val_score(eclf1, X_train, y_train, cv = 5)\n\nprint(\"Cross Validation Scores: \", scores)\nprint(\"Average CV Score: \", scores.mean())\nprint(\"Number of CV Scores used in Average: \", len(scores))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training the voting classifier, I evaluated its performance on the testing set to assess how well the model generalizes to new data. This step is important because it allows us to estimate the model's true performance on unseen data. I utilized various evaluation metrics such as accuracy, precision, recall, and F1-score. These metrics help us understand how well the model is performing in terms of correctly classifying income levels.","metadata":{"editable":false}},{"cell_type":"code","source":"y_pred = eclf1.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{"editable":false}},{"cell_type":"markdown","source":"After performing data exploration, preprocessing, model building, and evaluation, we can conclude that:\n\n1. The data contains information about individuals' demographic, education, and work-related attributes, which can be used to predict their income level.\n2. The dataset was preprocessed to handle missing values, encode categorical features, and scale the numerical features.\n3. We trained multiple models, including Decision Tree, Logistic Regression, and a Voting Classifier, to predict the income level.\n4. The Voting Classifier outperformed the other models in terms of accuracy, precision, recall, F1-score. It achieved an accuracy of 86.1% on the testing set.\n5. The Voting Classifier model can be used to predict income levels for individuals based on their demographic, education, and work-related attributes.\n\nOverall, the project demonstrates the importance of data exploration, preprocessing, and model selection in developing accurate and reliable machine learning models.","metadata":{"editable":false}}]}